{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMkjgPmy/Ez4OWP8vgX0X1I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/newjoseph/ML/blob/main/421test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### part 6 - application\n",
        "\n",
        "Now, we're going to estimate $\\beta$ in two ways. One with gradient descent and one with the exact method.\n",
        "\n",
        "We will use the `diabetes` dataset from sklearn to test our method. The code below will load this dataset and store the features in a matrix called `x` and the targets in a vector called `y`. Then we will split the data further into training and testing. \n",
        "\n",
        "First, lets write two functions in python\n",
        "\n",
        "1. Write a function called `loss_grad()` that takes a covariate matrix $X$, target vector $Y$, and a parameter vector $\\beta$ and computes $\\nabla \\ell(\\beta)$\n",
        "\n",
        "2. Write a function called `exact_beta_hat()` that takes a covariate matrix $X$ and target vector $Y$ and computes the exact solution. \n",
        "\n",
        "\n",
        "Now we will fit a linear model in two ways. \n",
        "\n",
        "1. Gradient descent. Write a gradient descent loop with your `loss_grad()` function to estimate $\\beta$ with `x_train` and `y_train` (don't forget to choose an appropriate $\\gamma$). Store this in a variable called `beta_gd`\n",
        "\n",
        "2. The exact method. Compute the exact estimate $\\hat \\beta$ with your function `exact_beta_hat()` on `x_train` and `y_train`and store the output in a variabled called `beta_exact`\n",
        "\n",
        "Compare `beta_gd` and `beta_exact`. Compare the MSE of each of your fitted models on the test data. \n",
        "\n",
        "$$\\ell(\\beta) = \\frac{1}{2}\\sum_{i=1}^n(y_i - x_i\\beta)^2 + C$$\n",
        "$$\\nabla \\ell(\\beta) = -2X^T(Y - X\\beta)$$\n",
        "$$\n",
        "\\hat \\beta = (X^TX)^{-1}X^TY\n",
        "$$"
      ],
      "metadata": {
        "id": "ugFTIMbdK7wR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSX4_LNnK6XA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import datasets\n",
        "\n",
        "x, y = datasets.load_diabetes(return_X_y = True)\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# gradient descent and exact soln\n",
        "\n",
        "def loss_grad(beta, x, y):\n",
        "  return torch.matmul(torch.t(x),(y - torch.matmul(x,beta)))\n",
        "  \n",
        "  #return -2*np.matmul(x.T,(y-np.matmul(x,beta)))\n",
        "  \n",
        "\n",
        "def exact_beta_hat(x, y): # (xTx)^-1(xTy)\n",
        "  return torch.matmul(torch.inverse(torch.matmul(torch.t(x),x)),torch.matmul(torch.t(x),y))\n",
        "  #return np.matmul(np.linalg.inv(np.matmul(x.T,x)),np.matmul(x.T,y))\n",
        "\n"
      ],
      "metadata": {
        "id": "lvF6Fxg9LIrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compare beta_gd and beta_exact\n",
        "\n",
        "\n",
        "beta_hat = torch.ones(10).double()\n",
        "beta_hat.requires_grad_()\n",
        "learning_rate = 1e-3\n",
        "\n",
        "beta_exact = exact_beta_hat(x_train, y_train)\n",
        "\n",
        "for _ in range(2000):\n",
        "  beta_gd = loss_grad(beta_hat, x_train, y_train)\n",
        "  #beta_gd.backward()\n",
        "  \n",
        "  beta_hat.data = beta_hat.data - 1e-3 * beta_hat.grad\n",
        "\n",
        "  beta_hat.grad.zero_()\n",
        "\n",
        "\n",
        "print(beta_gd,  beta_exact)"
      ],
      "metadata": {
        "id": "YUKM8vZ4LP4Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}